\documentclass[a4paper]{article}
\usepackage[dutch]{babel}
\usepackage{geometry}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{graphicx}
\usepackage[x11names, rgb, html]{xcolor}

% dimensions
\geometry{left=3cm, top=3cm, right=3cm, bottom=3cm}

% font
\usepackage{DejaVuSans}
\renewcommand*\familydefault{\sfdefault}
\usepackage[T1]{fontenc}

% hyperlinks
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  urlcolor=cyan,
}

% image
\graphicspath{{img/}}

% lists
\providecommand{\tightlist}{%
\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% preamble
\title{Experiments}
\author{Tim Visée \& Nathan Bakhuijzen}
\date{October 2018}

\begin{document}

  \pagenumbering{gobble}
  \maketitle
  \begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{cant-touch-this}
  \end{figure}
  \clearpage

  \section*{Preface}
  In this document, we will go through some experiments we've conducted for the
  platform we have built during the research minor KB-80. For each experiment
  we've written a new section describing what it is about, how we conduct the
  experiment and what our findings are. The general idea of conducing these
  experiments is to assert whether the current platform is \emph{usable} for
  the purpose we had originally in mind, namely controlling a generic computer.

  You may of course conduct similar experiments as we've described along with
  reading this document. Make sure you read the \emph{Manual} on how to install
  and use the product.

  The last part of the document goes over some ideas we came up with when
  working on on the project, and when conducting these experiments. These
  ideas may be implemented by others that continue the work on this project.

  \clearpage

  \section*{Accuracy \& Reliability}
  The accuracy and reliability of the gesture recognition is crucial to the
  succes of the platform. If it proves to be insufficient, then the platform is
  obsolete. No one will use it and time spent developing it will be wasted. If
  it is accurate and reliable however, then it may prove useful. It may be used
  to conduct futher research, and can be developed further.

  To test our platform, we came up with a list of predefined gestures. These
  gestures are programmed into the platform. This means that the predefined
  gesture is not jittery to begin with, as could be the case with recorded
  gestures. This is our list of predefined gestures:
  \begin{itemize}
    \tightlist
    \item Straight line
    \item Circle clockwise
    \item Circle counter-clockwise
    \item Big circle clockwise
    \item Big circle counter-clockwise
    \item Triangle clockwise
    \item Triangle counter-clockwise
    \item Mini square clockwise
    \item Square clockwise
    \item Square counter-clockwise
  \end{itemize}

  When we tested these gestures ourselves, we found that most of them work very
  well. The \textit{straight line} till the \textit{Big circle counter-clockwise}
  all work great. This is because the gestures have continual change. That means
  that the hand or fingers always move in a different direction than before.

  Gestures like the \textit{triangle} and \textit{square} work not as good as
  expected. These gestures have a tendency to not get recognized if the change
  in the direction is too great or after a long consistent pattern. This is
  because the gesture programmed into the platform has a specific check for
  change in directions. If the gesture deviates too much, the entire gesture
  will not be recognized. This is unfortunate, because that means that the
  gesture can be performed almost perfectly, only to be messed up at the end,
  resulting in a failure to recognize the gesture.

  This means that our gesture recognition can definitely be improved. Comparing
  gestures based on certain changes on a predefined distance is not effective.
  This means that the recognition has to be improved in order to implement even
  more complicated gestures.

  The current gesture recognition can be altered using the \textit{config.rs}
  file. By including a larger amount of points in scope, failure to recognize a
  gesture will be lower. This is because there will be more points, and more
  room for deviation from the predefined gesture.

  \section*{Gesture variety}
  % TODO: what gestures could be made
  % TODO: - can only use one finger, greatly limits possibilities
  % TODO: - we found out: sharp corners and long gestures don't work
  % TODO: - do we think the current system supports enough to control a computer
  % TODO: - what can we improve? (improve detection system, and how)

  \section*{Visual feedback}
  % TODO: answer: does visual feedback help, does it improve accuracy, what are
  % our experiences

  \section*{Recognition consistency}
  % TODO: we aren't performing gesture many times to improve accuracy
  It is important that all gestures are successfully performed and can be recognized
  multiple times. This is because recognizing gestures can be error-prone. By
  performing gestures often, the risk of it recognizing a gesture incorrectly
  decreases dramatically.

  \section*{Detection speed}
  Another important aspect of the gesture recognition system is the speed, or
  rather, the delay for detecting a performed gesture. To avoid confusion, we've
  decided to define the term delay as follows: \textit{The time it takes for the
    platform to show the name of the correct gesture in the console, after a
    gesture has been completely performed above the sensor}.

  \subsection*{Counting frames}
  Since it can be tricky to accurately measure the time difference between an
  invocation and an action, we've decided to record 20 invocations.
  With the resulting footage, we'll be able to count delay in
  frames, with precision accurately enough for our use case. The gesture that we
  choose was the square, as is has quite distinctive corners.

  \subsection*{Results}
  After performing the our own tests it was quite clear that delay varies
  greatly. The \emph{delay} varied from about 5 to 11 frames. With a speed of 60
  frames per second, each frame is about 17 milliseconds. This defines a delay
  varying between 85 to 187 milliseconds. There were two outliers with a delay
  of 16 to 22 frames which translate to about 270 and 370 milliseconds.

  We believe the varying difference has to do with the accuracy of detecting the
  gesture itself. Imagine you are drawing a perfect square gesture (perfect
  relative to the square gesture you might have previously recorded) above the
  sensor, the gesture will be recognized quicker before even fully completing
  the gesture, as compared to a square motion that differs slightly. Because
  it's virtually impossible to perform the gesture in exactly the same way every
  iteration, a difference in detection is indeed expected.

  Note that our method of testing latency is still quite simplistic. It doesn't
  take computer and screen delay into account. Nor does it test what the latency
  of the used sensor itself is. Our test strictly focusses on the delay between
  performing the gesture and seeing visual feedback. We performed the test with
  the visualizer enabled, which was visible on the material we recorded. What's
  interesting is that it looks like the gesture is instantly (meaning; within
  the same frame) recognized as the last sampled point of a gesture shows up on
  the visualizer. This would suggest that the actual latency for detection on
  the processed data is faster than 17 milliseconds. This is of course not very
  scientific, but we feel it's an awesome result non the less.

  What is important though, is how responsive the detection feels. During these
  tests, the detection (strictly speaking about detection speed) felt snappy even
  during the case of those outliers. The cool thing is that after you've
  performed a gesture you don't have to wait for a detection notification before
  the next gesture can be performed, that helps quite a lot for it to feel
  responsive to our opinion when when repeatedly experimenting with the sensor.

  The platform doesn't support binding
  generic actions to gestures to control a computer yet. And until that is
  tested, it's hard to say whether we'd feel the same when controlling a
  computer with these gestures. We mention this idea of binding actions later in
  this report in the \emph{Ideas} section.

  \section*{Ideas}
  During the project, and during the experiments we've conducted we came up with
  quite a few ideas. Some of them are things we'd wanted to implement but didn't
  have the time for it due to time constraints. Other ideas originate from
  test results we've collected.
  Here follow a few of them:

  \subsection*{Multiple fingers}
  The current system only has support for gestures consisting of a single trace,
  being the trace drawn by the index finger of a hand. The first logical
  expansion would of course be to support gestures having motions with 5
  fingers. Sadly, due to time constraints, we couldn't implement this.

  Our guess is that implementing this isn't too hard when using the same
  detection system as our current platform. The implementation might be as easy
  as extending our \verb_Hand_ structure (in code) to contain 5 traces, instead
  of just 1. This might be cool to experiment with for other groups taking a
  look at this project.

  \subsubsection*{Not just finger tips}
  With the extension described above, the system would still only be tracking
  finger tips. The sensor we're currently using is able to report the position
  of all joints in a hand, including the center of each bone and the center of
  the hand palm. This is easily visible in the 3D visualizer that is provided
  with the LeapMotion software package. The \verb_leap-rs_ wrapper that is
  currently used doesn't have an interface to obtain this information yet,
  but this could be implemented without too much effort.

  The same might be true for this implementation, just tracking the trace for
  every joint might be enough. We're of course not sure, that's something cool
  to test out.

  \subsection*{3D detection}
  Although we intended to support it at first, 3D gesture detection would be an
  awesome improvement. The current system uses 2D rotational data to represent
  user motions. Simply said; because of processing data to this format, the
  third dimension is lost. We found it difficult to reliably transform a 3D
  curve into 2D rotation data, and wanted to get the 2D system working first.

  Of course, motions in 3D space are transformed into data our platform uses.
  During this process the depth (Z-axis) is lost though. Drawing a circle
  sideways would be detected as moving up and down on a straight line. In this
  system, you might imagine drawing on a virtual plane on the X and Y axis.
  Supporting the Z-axis as well for a 3rd dimension would open up a lot of new
  possibilities for drawing gestures in different orientations and would support
  much more complex gestures.

  \subsection*{Bindable actions}
  Our idea was to allow users to bind custom actions to recorded gesture
  templates quite early in the project. This would allow a user to configure the
  platform to control their computer in a generic way. The implementation can be
  as simple as binding mouse (and scroll weel) actions, keys and system commands.

  Such an implementation would immediately make the project much more viable for
  use by others (given that it would work properly). This also opens new
  research questions such as; can gestures be complex enough to support enough
  gestures for controlling a generic computer?

  \paragraph{}
  This idea was also the segway to research on how usable a gesture detection
  system is for use in medical \& sterile environments. Imagine doctors not
  having to touch a röntgen machine to swipe between pictures, by using
  touch-less hand gestures instead. The implementation of bindable actions
  should make quite clear whether such a system is usable and reliable.

  \subsection*{Multiple sensors}
  An awesome extension would be to use and combine multiple sensors in the
  platform, instead of using a single LeapMotion sensor. The obvious choice
  would be to use a second LeapMotion sensor to combine data of the two. This
  could be used to filter jitter, and to fallback if the view for one sensor os
  occluded.

  This is one of the first things we looked into during the project. Sadly, it
  became apparent that the LeapMotion SDK doesn't support connecting multiple
  sensors, even though the provided library would suggest otherwise as there are
  functions available to obtain a list of connected sensors.
  \href{https://forums.leapmotion.com/t/multiple-leap-motion-support/770}{This}
  forum thread is about this topic, in which users are asking for support to use
  multiple sensors. In this thread, LeapMotion developers have said this is not
  possible.

  \paragraph{}
  It would be fun to use a different computer, or possible a virtual machine
  with a virtual USB controller to support connecting another sensor.
  The data from this sensor would then need to be
  transmitted to the platform to collect the sensor data. This is however
  outside the scope of our project, and is a challenge to properly implement on it's own.

  \paragraph{}
  Another option is to implement support for other kinds of sensors, such as the
  XBox Kinect, or something different. Our platform would than need to provide
  the proper abstractions to aggregate the data from different kinds of sensors
  into something the platform could use.

\end{document}

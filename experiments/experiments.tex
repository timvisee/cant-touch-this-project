\documentclass[a4paper]{article}
\usepackage[dutch]{babel}
\usepackage{geometry}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{graphicx}
\usepackage[x11names, rgb, html]{xcolor}

% dimensions
\geometry{left=3cm, top=3cm, right=3cm, bottom=3cm}

% font
\usepackage{DejaVuSans}
\renewcommand*\familydefault{\sfdefault}
\usepackage[T1]{fontenc}

% hyperlinks
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  urlcolor=cyan,
}

% image
\graphicspath{{img/}}

% lists
\providecommand{\tightlist}{%
\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% preamble
\title{Experiments}
\author{Tim Visée \& Nathan Bakhuijzen}
\date{October 2018}

\begin{document}

  \pagenumbering{gobble}
  \maketitle
  \begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{cant-touch-this}
  \end{figure}
  \clearpage

  \section*{Recognized motions}
  In this document, we will list all the motions that are recognized by
  \textit{Can't Touch This}. These experiments can be conducted by first setting
  up the platform itself. Instructions for this can be found in the manual.
  After setting everything up, conducting the experiments is a piece of cake.
  Start up the platform, open a webbrowser and go to \href{http://localhost:8000}.
  Click on `Visualize' to view your fingers being traced.

  \paragraph{}
  Please try to make the following gestures above the LeapMotion sensor. You
  will receive visual feedback if a gesture has been made succesfully.

  \begin{itemize}
    \tightlist
    \item Straight line
    \item Circle clockwise
    \item Circle counter-clockwise
    \item Big circle clockwise
    \item Big circle counter-clockwise
    \item Triangle clockwise
    \item Triangle counter-clockwise
    \item Mini square clockwise
    \item Square clockwise
    \item Square counter-clockwise
  \end{itemize}

  \section{Visual feedback}
  % TODO

  \section*{Recognition consistency}
  % TODO: we aren't performing gesture many times to improve accuracy
  It is important that all gestures are successfully performed and can be recognized
  multiple times. This is because recognizing gestures can be error-prone. By
  performing gestures often, the risk of it recognizing a gesture incorrectly
  decreases dramatically.

  \section*{Detection speed}
  Another important aspect of the gesture recognition system is the speed, or
  rather, the delay for detecting a performed gesture. To avoid confusion, we've
  decided to define the term delay as follows: \textit{The time it takes for the
    platform to show the name of the correct gesture in the console, after a
    gesture has been completely performed above the sensor}.

  \subsection*{Counting frames}
  Since it can be tricky to accurately measure the time difference between an
  invocation and an action, we've decided to record 20 invocations.
  With the resulting footage, we'll be able to count delay in
  frames, with precision accurately enough for our use case. The gesture that we
  choose was the square, as is has quite distinctive corners.

  \subsection{Results}
  After performing the our own tests it was quite clear that delay varies
  greatly. The \emph{delay} varied from about 5 to 11 frames. With a speed of 60
  frames per second, each frame is about 17 milliseconds. This defines a delay
  varying between 85 to 187 milliseconds. There were two outliers with a delay
  of 16 to 22 frames which translate to about 270 and 370 milliseconds.

  We believe the varying difference has to do with the accuracy of detecting the
  gesture itself. Imagine you are drawing a perfect square gesture (perfect
  relative to the square gesture you might have previously recorded) above the
  sensor, the gesture will be recognized quicker before even fully completing
  the gesture, as compared to a square motion that differs slightly. Because
  it's virtually impossible to perform the gesture in exactly the same way every
  iteration, a difference in detection is indeed expected.

  Note that our method of testing latency is still quite simplistic. It doesn't
  take computer and screen delay into account. Nor does it test what the latency
  of the used sensor itself is. Our test strictly focusses on the delay between
  performing the gesture and seeing visual feedback. We performed the test with
  the visualizer enabled, which was visible on the material we recorded. What's
  interesting is that it looks like the gesture is instantly (meaning; within
  the same frame) recognized as the last sampled point of a gesture shows up on
  the visualizer. This would suggest that the actual latency for detection on
  the processed data is faster than 17 milliseconds. This is of course not very
  scientific, but we feel it's an awesome result non the less.

  What is important though, is how responsive the detection feels. During these
  tests, the detection (strictly speaking about detection speed) felt snappy even
  during the case of those outliers. The cool thing is that after you've
  performed a gesture you don't have to wait for a detection notification before
  the next gesture can be performed, that helps quite a lot for it to feel
  responsive to our opinion when when repeatedly experimenting with the sensor.

  The platform doesn't support binding
  generic actions to gestures to control a computer yet. And until that is
  tested, it's hard to say whether we'd feel the same when controlling a
  computer with these gestures. We mention this idea of binding actions later in
  this report in the \emph{Ideas} section.

  \section{Ideas}
  During the project, and during the experiments we've conducted we came up with
  quite a few ideas. Some of them are things we'd wanted to implement but didn't
  have the time for it due to time constraints. Other ideas originate from
  test results we've collected.
  Here follow a few of them:

  \subsection{Multiple fingers}
  The current system only has support for gestures consisting of a single trace,
  being the trace drawn by the index finger of a hand. The first logical
  expansion would of course be to support gestures having motions with 5
  fingers. Sadly, due to time constraints, we couldn't implement this.

  Our guess is that implementing this isn't too hard when using the same
  detection system as our current platform. The implementation might be as easy
  as extending our \verb_Hand_ structure (in code) to contain 5 traces, instead
  of just 1. This might be cool to experiment with for other groups taking a
  look at this project.

  \subsubsection{Not just finger tips}
  With the extension described above, the system would still only be tracking
  finger tips. The sensor we're currently using is able to report the position
  of all joints in a hand, including the center of each bone and the center of
  the hand palm. This is easily visible in the 3D visualizer that is provided
  with the LeapMotion software package. The \verb_leap-rs_ wrapper that is
  currently used doesn't have an interface to obtain this information yet,
  but this could be implemented without too much effort.

  The same might be true for this implementation, just tracking the trace for
  every joint might be enough. We're of course not sure, that's something cool
  to test out.

  \subsection{3D detection}
  Although we intended to support it at first, 3D gesture detection would be an
  awesome improvement. The current system uses 2D rotational data to represent
  user motions. Simply said; because of processing data to this format, the
  third dimension is lost. We found it difficult to reliably transform a 3D
  curve into 2D rotation data, and wanted to get the 2D system working first.

  Of course, motions in 3D space are transformed into data our platform uses.
  During this process the depth (Z-axis) is lost though. Drawing a circle
  sideways would be detected as moving up and down on a straight line. In this
  system, you might imagine drawing on a virtual plane on the X and Y axis.
  Supporting the Z-axis as well for a 3rd dimension would open up a lot of new
  possibilities for drawing gestures in different orientations and would support
  much more complex gestures.

  \subsection{Bindable actions}
  Our idea was to allow users to bind custom actions to recorded gesture
  templates quite early in the project. This would allow a user to configure the
  platform to control their computer in a generic way. The implementation can be
  as simple as binding mouse (and scroll weel) actions, keys and system commands.

  Such an implementation would immediately make the project much more viable for
  use by others (given that it would work properly). This also opens new
  research questions such as; can gestures be complex enough to support enough
  gestures for controlling a generic computer?

  \paragraph{}
  This idea was also the segway to research on how usable a gesture detection
  system is for use in medical \& sterile environments. Imagine doctors not
  having to touch a röntgen machine to swipe between pictures, by using
  touch-less hand gestures instead. The implementation of bindable actions
  should make quite clear whether such a system is usable and reliable.

  \subsection{Multiple sensors}
  An awesome extension would be to use and combine multiple sensors in the
  platform, instead of using a single LeapMotion sensor. The obvious choice
  would be to use a second LeapMotion sensor to combine data of the two. This
  could be used to filter jitter, and to fallback if the view for one sensor os
  occluded.

  This is one of the first things we looked into during the project. Sadly, it
  became apparent that the LeapMotion SDK doesn't support connecting multiple
  sensors, even though the provided library would suggest otherwise as there are
  functions available to obtain a list of connected sensors.
  \href{https://forums.leapmotion.com/t/multiple-leap-motion-support/770}{This}
  forum thread is about this topic, in which users are asking for support to use
  multiple sensors. In this thread, LeapMotion developers have said this is not
  possible.

  \paragraph{}
  It would be fun to use a different computer, or possible a virtual machine
  with a virtual USB controller to support connecting another sensor.
  The data from this sensor would then need to be
  transmitted to the platform to collect the sensor data. This is however
  outside the scope of our project, and is a challenge to properly implement on it's own.

  \paragraph{}
  Another option is to implement support for other kinds of sensors, such as the
  XBox Kinect, or something different. Our platform would than need to provide
  the proper abstractions to aggregate the data from different kinds of sensors
  into something the platform could use.

\end{document}
